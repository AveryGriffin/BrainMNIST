{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import copy\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_data = pd.read_csv(\"C:/Users/Avery/Anaconda3/envs/mnistbrain/EP1.01.txt\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_data.columns = [\"ID\", \"Event ID\", \"Device\", \"Channel\", \"Label\", \"HzCaptured\", \"Signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through the data and extracts all of the unique labels.\n",
    "def get_labels_in_set(dataset, labelcolname=\"Label\"):\n",
    "    listoflabels = []\n",
    "    for i in range(len(dataset)):\n",
    "        newlabel = dataset[labelcolname][i]\n",
    "        if newlabel not in listoflabels:\n",
    "            listoflabels.append(newlabel)\n",
    "    return listoflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Labels in IN Dataset\n",
    "listoflabels = get_labels_in_set(EP_data, \"Label\")\n",
    "print(\"Labels In Data Set: \", sorted(listoflabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data by Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out all observations with <250 Hz Captured. Needed for the 250 nodes in the input layer of our neural network.\n",
    "def filter_data(dataset, Hz_colname, filternum=250):\n",
    "    data_filter = dataset[dataset[Hz_colname] >= filternum]\n",
    "    print(\"lengths of dataset\", len(dataset),\n",
    "          \"length of filtered set:\", len(data_filter),\n",
    "          \"length that was filtered out:\", len(dataset[dataset[Hz_colname] < filternum]))\n",
    "    print(\"Percent of Original Data Retained:\", round(len(data_filter) / len(dataset) * 100, 2), \"%\")\n",
    "    return data_filter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_data_filter = filter_data(EP_data, \"HzCaptured\", 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data by -1 Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_negatives(dataset, label_colname):\n",
    "    data_filter = dataset[dataset[label_colname] != -1]\n",
    "    print(\"lengths of dataset\", len(dataset),\n",
    "          \"length of filtered set\", len(data_filter),\n",
    "          \"length that was filtered out\", len(dataset[dataset[label_colname] == -1]))\n",
    "    print(\"Percent of Original Data Retained\", round(len(data_filter) / len(dataset) * 100, 2), \"%\")\n",
    "    return data_filter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_data_filter = filter_negatives(EP_data_filter, \"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Signal Strings to Arrays of Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_data_array = EP_data_filter.to_numpy()\n",
    "print(EP_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts the string signal data into an array and then converts each string element into a float element.\n",
    "def string_to_float(stringed_signal_data):\n",
    "    float_signal_data = np.empty((len(stringed_signal_data)), dtype=object)\n",
    "    for n in range(len(float_signal_data)):\n",
    "        single_signal_observation = stringed_signal_data[n][6].split(\",\")\n",
    "        single_signal_observation = [float(i) for i in single_signal_observation]        \n",
    "        float_signal_data[n] = single_signal_observation\n",
    "    print(\"The Shape of the Array we created after converting to floats:\", float_signal_data.shape)\n",
    "    print(\"The Shape of the Original Array of Stringed Signal Data:     \", stringed_signal_data[:,6].shape)\n",
    "\n",
    "    return float_signal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_signal_float = string_to_float(EP_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_data_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping the Signal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_signal(signal_float, length):\n",
    "    signal_cropped = np.empty((len(signal_float)), dtype=object)\n",
    "    for n in range(len(signal_cropped)):\n",
    "        signal_cropped[n] = np.array(signal_float[n][:250])\n",
    "    print(\"Shape of 1st sample in signal\", signal_cropped[0].shape)\n",
    "    priant(\"Shape of all signal data\", signal_cropped.shape)\n",
    "    return signal_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_signal_cropped = crop_signal(EP_signal_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_signal_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping X (by event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This groups everything. First, create an empty array, then create mini arrays of c observations grouped together.\n",
    "def group_x(x_,channels):\n",
    "    grouped_x = np.empty((len(x_)//channels), dtype=object)\n",
    "    temp_x = []\n",
    "    for i in range(len(x_)):\n",
    "        temp_x.append(x_[i])\n",
    "        if (i+1) % channels == 0:\n",
    "            j = ((i+1)//channels)-1\n",
    "            grouped_x[j] = np.array(temp_x)\n",
    "            temp_x = []\n",
    "    \n",
    "    print(\"Check if everything is of type array:\", type(grouped_x), type(grouped_x[10]), type(grouped_x[15][9]))\n",
    "    print(\"Shape of Grouped X:\", grouped_x.shape, \"\\nShape of Original X Divided by C:\", len(x_)//channels)\n",
    "    \n",
    "    return grouped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_grouped_x = group_x(EP_signal_cropped, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_signal_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_grouped_x.dtype # Tensorflow can't handle Objects \"O\" dtypes. Luckily this is automatically fixed when flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_shape(x_, new_shape_order):\n",
    "    if new_shape_order == \"channel-time\":\n",
    "        copy_x = []\n",
    "        for i in x_:\n",
    "            copy_x.append(i)\n",
    "        copy_x = np.array(copy_x)\n",
    "    elif new_shape_order == \"time-channel\":\n",
    "        copy_x = []\n",
    "        for i in x_:\n",
    "            copy_x.append(i.T)\n",
    "        copy_x = np.array(copy_x)\n",
    "    elif new_shape_order == \"flattened\":\n",
    "        copy_x = []\n",
    "        for i in x_:\n",
    "            copy_x.append(i.T.flatten())\n",
    "        copy_x = np.array(copy_x)\n",
    "    \n",
    "    print(\"New Shape:\", copy_x.shape)\n",
    "    print(\"Sample of X:\", copy_x[13])\n",
    "    return copy_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_x = change_shape(EP_grouped_x, \"flattened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_grouped_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EP_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array(([1.0,2.0,3.0],\n",
    "          [4.0,5.0,6.0],\n",
    "          [7.0,1.0,14.0],\n",
    "          [10.0,11.0,12.0]))\n",
    "print(temp_x.shape)\n",
    "print(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(x_prescaled):\n",
    "    for feature in range(x_prescaled.T.shape[0]):\n",
    "        feature_min = min(x_prescaled.T[feature])\n",
    "        feature_max = max(x_prescaled.T[feature])\n",
    "        feature_range = feature_max - feature_min\n",
    "        x_prescaled.T[feature] = ( x_prescaled.T[feature] - feature_min ) / feature_range\n",
    "    return x_prescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verified that this works properly\n",
    "scaled_x = scale_features(EP_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize(x_prenormalized):\n",
    "    for feature in range(x_prenormalized.T.shape[0]):\n",
    "        feature_mean = sum(x_prenormalized.T[feature]) / len(x_prenormalized.T[feature])\n",
    "        x_prenormalized.T[feature] = x_prenormalized.T[feature] - feature_mean\n",
    "    return x_prenormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verified that this works properly.\n",
    "normalized_x = mean_normalize(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaled_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_x(pre_x, channels, timepoints):\n",
    "    new_x = np.reshape(pre_x, (pre_x.shape[0], channels, timepoints))\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_x = reshape_x(normalized_x, 14, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Hot Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(dataarray_set, c):\n",
    "    y_preprocess = dataarray_set[:,4]\n",
    "    print(\"Y Original Length:\", len(y_preprocess), \"\\nExamples of Y:\", y_preprocess[:30])\n",
    "    \n",
    "    y_divided = [y_preprocess[i] for i in range(len(y_preprocess)) if (i+1)%c == 0] # Extract 1 label per event instead of c\n",
    "    print(\"\\n Y Length after filtering out event duplicates (dividing by c):\", len(y_divided))\n",
    "    print(\"Examples of Y after filtering:\", y_divided[:30])\n",
    "    \n",
    "    return y_divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an equivalent array of arrays using the encoding system.\n",
    "def encode_hot_y(dataarray_set, labelslist, c):\n",
    "    \n",
    "    y_empty = np.empty((len(dataarray_set)//c), dtype=object)\n",
    "    \n",
    "    for i in range(len(y_empty)):\n",
    "        y_empty[i] = np.zeros((len(labelslist)-1), int)\n",
    "        \n",
    "    y_ = create_y(dataarray_set, c)\n",
    "    \n",
    "    print(\"\\n What y array looks like before assigning 1s: \\n\", y_empty[:10])\n",
    "    \n",
    "    for i in range(len(y_empty)): # This encodes the 1 for each label\n",
    "        n = y_[i]\n",
    "        y_empty[i][n] = 1\n",
    "        \n",
    "    print(\"What y array looks like after assigning 1s: \\n\", y_empty[:10])\n",
    "    \n",
    "    return y_empty\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_encode_y = encode_hot_y(EP_data_array, listoflabels, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dtype(y_):\n",
    "    print(\"Previous Dtype:\", y_.dtype)\n",
    "    if y_.dtype == 'O':\n",
    "        y_ = np.vstack(y_[:]).astype(np.float)\n",
    "        print(\"Fixed. New Dtye:\", y_.dtype)\n",
    "        return y_\n",
    "    else:\n",
    "        return \"Not Object Type:\", y_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_y = fix_dtype(EP_encode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_encode_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EP_x.nbytes/(1024**3), \"GB\")\n",
    "print(EP_y.nbytes/(1024**3), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(x, y, dataName):\n",
    "    with h5py.File(dataName + '_x.h5', 'w') as hf:\n",
    "        hf.create_dataset(dataName + \"_x_dataset\", data=x)\n",
    "        \n",
    "    with h5py.File(dataName + \"_y.h5\", \"w\") as hf:\n",
    "        hf.create_dataset(dataName + \"_y_dataset\", data=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(EP_x, EP_y, \"EP_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EP_x\n",
    "del EP_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
